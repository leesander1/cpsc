{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "221d8e19",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "---\n",
    "\n",
    "## *Section 1: N-Grams*\n",
    "\n",
    "**Concept:** N-grams are contiguous sequences of \"n\" items (typically\n",
    "words) from a given sample of text. Commonly used n-grams are unigrams\n",
    "(n=1), bigrams (n=2), and trigrams (n=3). N-grams are foundational in NLP, especially in\n",
    "language modeling, text generation, and context-based analysis.\n",
    "\n",
    "**How it works:**\n",
    "To extract n-grams from a text, a sliding window of size `n` moves across the tokenized text. Each time the window moves, it captures a new set of `n` adjacent tokens. Frequency distributions of these n-grams can then be computed to identify common patterns or sequences.\n",
    "\n",
    "**Demonstration:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c620f4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "texts = newsgroups.data\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), max_features=20)\n",
    "X = vectorizer.fit_transform(texts[:1000])\n",
    "bigrams = vectorizer.get_feature_names_out()\n",
    "\n",
    "plt.barh(bigrams, X.toarray().sum(axis=0))\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.title(\"Top Bigrams\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c13e9e",
   "metadata": {},
   "source": [
    "**Exercise 1:** The previous visualization has limited uses because of the inclusion of stop words. Remove the stop words and visualize the top 20 trigrams from the first 1000 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8525135a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "111bb1f8871018de74baaf600ee88a64",
     "grade": false,
     "grade_id": "cell-7ac3b4afbcbd2fe0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815c1101",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72df62fb3e45af4c5abaee3e7b0b228e",
     "grade": true,
     "grade_id": "cell-32b808ed8f14ad07",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7450cea",
   "metadata": {},
   "source": [
    "**Exercise 2:** Compare the frequency distribution of bigrams between the newsgroup categories `sci.space` and `rec.sport.hockey`. (check out the lda demo code for how categories are defined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fee3420",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75596bd0be3ea98e2a17c66a2ff4e6f3",
     "grade": false,
     "grade_id": "cell-6e90893e5b67c5c1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i, cat in enumerate(cats):\n",
    "    mask = labels == i\n",
    "    cat_counts = X[mask].sum(axis=0).A1\n",
    "    plt.bar([x + i * 0.3 for x in range(20)], cat_counts, width=0.3, label=cat)\n",
    "\n",
    "plt.xticks(range(20), vectorizer.get_feature_names_out(), rotation=90)\n",
    "plt.legend()\n",
    "plt.title(\"Top Bigrams by Category\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a44643",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "73cfb114036e266677b1d1c3b86b63ab",
     "grade": true,
     "grade_id": "cell-29d8d961c5faedb9",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bd97d3e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## *Section 2: Named Entity Recognition (NER)*\n",
    "\n",
    "**Concept:** NER locates and classifies entities in text into predefined categories like persons, organizations, locations, dates, etc. Entities provide structure and semantic understanding to raw text, enabling more meaningful search, summarization, and recommendation systems. NER uses pre-trained models that rely on statistical or neural network-based approaches to detect entities. Tokens are passed through a model which assigns them a label such as `PERSON`, `ORG`, or `DATE`. These models often use contextual embeddings to make predictions. The demo here uses statistical models, but the interpretation remains the same.\n",
    "\n",
    "**Demonstration:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6a7e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(texts[0])\n",
    "spacy.displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9eaef2",
   "metadata": {},
   "source": [
    "**Exercise 1:** Extract all unique organizations from the first 100\n",
    "documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee9890d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6839089be84dc421edb75a41d290508c",
     "grade": false,
     "grade_id": "cell-700ab7289a0632dc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "orgs = set()\n",
    "# your code here\n",
    "raise NotImplementedError\n",
    "print(orgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7155b16",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f475c0ef15b1c145980238e1bc6935d",
     "grade": true,
     "grade_id": "cell-9592535003c4fc5b",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bbe0de5",
   "metadata": {},
   "source": [
    "**Exercise 2:** Count and visualize the frequency of each named entity\n",
    "label in the first 200 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e57cf42",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e33f4a9ef8273524884e7fecd1432d5c",
     "grade": false,
     "grade_id": "cell-7b77f1b88c1727b4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5981652",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0586ebaef126a169b13029304cb9c99",
     "grade": true,
     "grade_id": "cell-7cca70f06f115d63",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9024bcd9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## *Section 3: Dependency Parsing*\n",
    "\n",
    "**Concept:** Dependency parsing uncovers grammatical structure and relationships between words, identifying subjects, objects, and modifiers. Parsing is key to  understanding sentence-level meaning and grammar for complex NLP applications.\n",
    "\n",
    "**How it works:**\n",
    "Dependency parsers analyze the grammatical structure of a sentence by identifying the \"head\" word of each word in the sentence and labeling the relationships (dependencies) between them. This generates a tree structure representing grammatical roles such as `nsubj` (nominal subject) or `dobj` (direct object).\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "-   Grammar checking tools\n",
    "-   Semantic search systems\n",
    "-   Intelligent chatbots\n",
    "\n",
    "**Demonstration:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81969b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "doc = nlp(sentence)\n",
    "spacy.displacy.render(doc, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392e56d2",
   "metadata": {},
   "source": [
    "**Exercise 1:** Count how many times a noun serves as a subject (nsubj) in\n",
    "the first 200 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7949fe",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50a91f48fb61a4cc358aa05ea930fd98",
     "grade": false,
     "grade_id": "cell-161f7f59684ecb47",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "nsubj_count = 0\n",
    "# your code here\n",
    "raise NotImplementedError\n",
    "print(nsubj_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88aee53",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5e724081f281478c078ddacbae49361",
     "grade": true,
     "grade_id": "cell-28b6ebcae4d9d09c",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b4f96ab",
   "metadata": {},
   "source": [
    "**Exercise 2:** Find and visualize the top 10 most frequent dependency labels in the dataset. (and review what these parts of speech mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f43511",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "465f0c8d2b08c50c879e6f9ce8ce7d31",
     "grade": false,
     "grade_id": "cell-c4e8d9456c6518f0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b32889f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "39724fcedb2cd907668df6530dcfc9af",
     "grade": true,
     "grade_id": "cell-69bfb5f7a0706ea8",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10f7507d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## *Section 4: Sentiment Analysis*\n",
    "\n",
    "**Concept:** Sentiment analysis assigns emotional tone to text, commonly classifying as positive, negative, or neutral. Understanding sentiment helps organizations evaluate user opinions, reviews, or public reactions.\n",
    "\n",
    "**How it works:**\n",
    "Sentiment analysis often uses a combination of rule-based systems and machine learning models. Rule-based systems like VADER assign polarity scores to words and aggregate them, considering intensifiers and negations. Machine learning approaches use annotated corpora to learn patterns associated with sentiment.\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "-   Product review classification\n",
    "-   Social media monitoring\n",
    "-   Political campaign analysis\n",
    "\n",
    "**Demonstration:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f419ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "score = sia.polarity_scores(texts[0])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15df08a",
   "metadata": {},
   "source": [
    "**Exercise 1:** Count how many of the first 100 texts are mostly positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92dd87a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7ee5e50b0684a4a57d3e9dd7f7826de",
     "grade": false,
     "grade_id": "cell-822561ab4ec604e4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError\n",
    "print(positive_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5108c2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a87ab7c98d2f1385c9cd370206f7dcf",
     "grade": true,
     "grade_id": "cell-cacc3a7e348385fe",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd0065e1",
   "metadata": {},
   "source": [
    "**Exercise 2:** Plot the sentiment distribution (positive, negative,\n",
    "neutral) for the first 500 texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec6e8ae",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "359946584de873d55adda584a859b427",
     "grade": false,
     "grade_id": "cell-012c4cb3e6429939",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8bf112",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "392bef9ea0bfd1bb4d5c995e55e63245",
     "grade": true,
     "grade_id": "cell-af26c33d200408b0",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd0d5a09",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## *Section 5: Topic Modeling with LDA*\n",
    "\n",
    "**Concept:** LDA is an unsupervised algorithm that finds abstract topics\n",
    "in a corpus based on word distribution.\n",
    "\n",
    "**How it works:**\n",
    "Latent Dirichlet Allocation assumes each document is a mixture of topics, and each topic is a mixture of words. The algorithm infers topic distributions by iteratively updating topic assignments for each word using probabilistic inference. It uses Dirichlet priors to ensure sparsity in topic and word distributions.\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "-   News topic categorization\n",
    "-   Academic literature review\n",
    "-   Legal discovery\n",
    "\n",
    "**Demonstration:** See lda notebook on canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1512ffb2",
   "metadata": {},
   "source": [
    "**Exercise 1:** Print the top 10 words for each of 7 topics from the `rec.autos` category.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f67b22b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "473ca12f033a2f0af431e50e0e7503e6",
     "grade": false,
     "grade_id": "cell-bf7ff1930fdb61ce",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9007986",
   "metadata": {},
   "source": [
    "**Exercise 2:** Visualize your LDA model using pyLDAvis. (check out the lda demo code for how categories are defined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070a0f4c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88b573951be90eaf173df07a5688b514",
     "grade": false,
     "grade_id": "cell-f1a2ba5be703e963",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
